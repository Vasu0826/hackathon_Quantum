{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference Chat - https://chat.openai.com/share/cb88c8d6-90d6-44da-ba5b-668bfcb096f7\n"
      ],
      "metadata": {
        "id": "BXNV9oqultLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll simulate a small dataset of transactions, each with a few basic features:\n",
        "\n",
        "Transaction ID: A unique identifier for each transaction.\n",
        "\n",
        "Amount: The amount of money involved in the transaction.\n",
        "\n",
        "Type: The type of transaction (e.g., deposit, withdrawal).\n",
        "\n",
        "Location: The geographic location of the transaction.\n",
        "\n",
        "Time: The time at which the transaction occurred.\n",
        "\n",
        "IsFraud: A flag indicating whether the transaction is fraudulent (for the sake of example, we'll include this, though in a real scenario, this would be unknown and what the model aims to predict).\n",
        "\n",
        "For the pre-filtering step, we'll perform:\n",
        "\n",
        "Removal of outliers based on the transaction amount.\n",
        "Filtering based on transaction type, focusing perhaps on more commonly fraudulent types.\n",
        "Basic cleaning, like filling in missing values if necessary (our sample will not include missing values for simplicity)."
      ],
      "metadata": {
        "id": "jd6VHV55SMb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocjZmkq9RPk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a7f0e7-2091-41bc-967b-d766ec74388f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2485.5075 7467.8475\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset generation\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Generating sample data\n",
        "df = pd.read_csv('transactions.csv')\n",
        "\n",
        "\n",
        "# Pre-filtering steps\n",
        "# 1. Removal of outliers in transaction amount\n",
        "q1, q3 = np.percentile(df[\"Amount\"], [25, 75])\n",
        "iqr = q3 - q1\n",
        "lower_bound = q1 - (1.5 * iqr)\n",
        "upper_bound = q3 + (1.5 * iqr)\n",
        "\n",
        "print(q1,q3)\n",
        "\n",
        "filtered_df = df[(df[\"Amount\"] > lower_bound) & (df[\"Amount\"] < upper_bound)]\n",
        "\n",
        "# 2. Filtering based on transaction type (e.g., focusing on 'withdrawal' and 'transfer' types)\n",
        "filtered_df = filtered_df[filtered_df[\"Type\"].isin([\"withdrawal\", \"transfer\"])]\n",
        "\n",
        "# Displaying the original and filtered datasets\n",
        "filtered_df.to_csv('filtered_transactions.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Step 2, Feature Extraction, the goal is to transform raw data into meaningful features that can be used by machine learning models to detect fraudulent activities in e-banking transactions. Given the sample dataset structure provided earlier, we will focus on extracting and engineering features that could be significant for identifying potential fraud.\n",
        "\n",
        "Feature Engineering Ideas\n",
        "Time Features: Fraudulent activities might occur more frequently during certain hours. Extract features like the hour of the day or the day of the week from the transaction time.\n",
        "\n",
        "Amount Features: Since the transaction amount could be indicative of fraud, consider normalizing or binning this feature. For example, transactions can be categorized into small, medium, and large based on amount thresholds.\n",
        "\n",
        "Location-Based Features: If certain locations are more prone to fraud, create dummy variables for each location type (e.g., Online, Branch, ATM).\n",
        "\n",
        "Type-Based Features: Similarly, transaction types could be relevant. Create dummy variables for each transaction type (e.g., deposit, withdrawal, transfer)."
      ],
      "metadata": {
        "id": "DZ68dOFpZPRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# taking filtered data\n",
        "df_filtered = pd.read_csv('filtered_transactions.csv')\n",
        "df_filtered['Time'] = pd.to_datetime(df['Time'])\n",
        "\n",
        "# 1. Time Features\n",
        "df_filtered['HourOfDay'] = df_filtered['Time'].dt.hour\n",
        "df_filtered['DayOfWeek'] = df_filtered['Time'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "\n",
        "# 2. Amount Features: Example of binning into categories\n",
        "amount_bins = [-1, 1000, 5000, np.inf]  # Bins for the transaction amount\n",
        "amount_labels = ['small', 'medium', 'large']\n",
        "df_filtered['AmountCategory'] = pd.cut(df_filtered['Amount'], bins=amount_bins, labels=amount_labels)\n",
        "\n",
        "# 3. Location-Based Features: Creating dummy variables\n",
        "location_dummies = pd.get_dummies(df_filtered['Location'], prefix='Loc')\n",
        "df_filtered = pd.concat([df_filtered, location_dummies], axis=1)\n",
        "\n",
        "# 4. Type-Based Features: Creating dummy variables\n",
        "type_dummies = pd.get_dummies(df_filtered['Type'], prefix='Type')\n",
        "df_filtered = pd.concat([df_filtered, type_dummies], axis=1)\n",
        "\n",
        "# Dropping the original 'Time', 'Location', 'Type', 'Amount' columns if they are no longer needed\n",
        "# This is optional and depends on whether you want to keep the original features alongside the engineered ones\n",
        "df_filtered.drop(['Time', 'Location', 'Type', 'Amount'], axis=1, inplace=True)\n",
        "\n",
        "# Display the DataFrame with new features\n",
        "df_filtered.to_csv('Feature_extraction_transactions.csv', index=False)"
      ],
      "metadata": {
        "id": "3F4ImxrJZISJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For Step 3, focusing on Machine Learning for Fraud Detection, we'll implement a simple model using classical machine learning techniques as a baseline. Although the ultimate goal is to incorporate Quantum Machine Learning (QML) methods, establishing a classical baseline is a useful first step for comparison and to ensure the feature engineering process is effective.\n",
        "\n",
        "We'll use the processed transactions data with extracted features to train a machine learning model. For this example, let's use a Random Forest Classifier, known for its effectiveness in classification tasks, including fraud detection.\n",
        "\n",
        "Load the Processed Data: Start by loading the processed data from the CSV file created in the previous step.\n",
        "\n",
        "Prepare the Data: Split the data into features (X) and the target variable (y), which is the IsFraud column in this case.\n",
        "\n",
        "Split the Data: Divide the dataset into training and testing sets to evaluate the model's performance.\n",
        "\n",
        "Train the Model: Train a Random Forest Classifier on the training data.\n",
        "Evaluate the Model: Assess the model's performance on the testing set using appropriate metrics like accuracy, precision, recall, and the F1 score."
      ],
      "metadata": {
        "id": "cBtsTYiziZBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Load the Processed Data\n",
        "df = pd.read_csv('Feature_extraction_transactions.csv')  # Update this path\n",
        "\n",
        "# Step 2: Prepare the Data\n",
        "X = df.drop(['TransactionID', 'IsFraud'], axis=1)  # Features\n",
        "y = df['IsFraud']  # Target variable\n",
        "\n",
        "# Convert categorical variables (if any) into dummy/indicator variables\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Step 3: Split the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Step 4: Train the Model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy,\n",
        "    \"Precision\": precision,\n",
        "    \"Recall\": recall,\n",
        "    \"F1 Score\": f1\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame([metrics])\n",
        "\n",
        "metrics_df.to_csv('final_result.csv', index=False)"
      ],
      "metadata": {
        "id": "ZgczAAU2iqg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy measures the overall correctness of the model.\n",
        "Precision is the ratio of true positive predictions to the total positive predictions (important to minimize false positives).\n",
        "Recall measures how well the model can find the positive samples (important for not missing actual fraud cases).\n",
        "F1 Score is the harmonic mean of precision and recall, providing a balance between the two."
      ],
      "metadata": {
        "id": "RBDNYqNJi54U"
      }
    }
  ]
}